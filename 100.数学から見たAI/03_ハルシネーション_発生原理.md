# ハルシネーションはなぜ起きるのか  
― 確率空間から見る原因整理 ―

本ドキュメントでは、  
生成AI（ChatGPT / Amazon Q 等）が **なぜ事実ではない回答（ハルシネーション）を生成するのか** を、  
「確率空間」という観点から説明する。

内部実装・学習手法・数式には踏み込まない。

---

## 結論（先に）

> **ハルシネーションは「嘘」ではない。  
> 確率空間の中で、  
> “それらしいが根拠のない候補”が  
> 選ばれてしまった結果である。**

---

## AIは何をしているのか（前提）

AIは常に次の処理を行っている。

> **次に出力し得る候補すべてに確率を割り当て、  
> 最も尤もらしいものを選択する**

重要なのは：

- 正解かどうか → 判断していない  
- 事実かどうか → 確認していない  
- **確率が高いかどうか** → それだけを見ている  

---

## 確率空間とは何か

確率空間とは、

> **AIが「出してよい」と考えている  
> 回答候補の集合**

である。

この空間には最初、以下がすべて含まれている。

- 正確な事実
- 曖昧な説明
- 一般論
- 推測
- 創作的補完
- 誤った知識

**AIはこれらを区別していない**  
すべてが「候補」として存在している

---

## ハルシネーションが起きる本質的理由

### ① 確率空間が広すぎる

制約が弱い場合、

- 推測
- 一般論
- それっぽい創作

が **排除されない**。

その結果、

> **事実よりも  
> “それらしく聞こえる説明”の確率が高くなる**

---

### ② 「分からない」という候補の確率が低い

多くの確率空間では、

- 「不明」
- 「情報不足」
- 「判断できない」

と答える候補は、  
**初期状態では確率が低い**。

制約で明示しない限り、

> **AIは沈黙より“何か言う”を選ぶ**

それが人間にとって嘘に見える。

---

### ③ 参照情報（RAG）がノイズを含んでいる

RAGに以下が混ざると

- 人間向け背景説明
- 設計意図
- 「通常は」「一般的に」
- 古い仕様

確率空間では
- 無関係な文脈にも確率が割り当てられる
- 候補が増え、分布が汚れる

**事実候補の相対確率が下がり、品質が低下する**

---

### ④ AIは「正しさ」を評価していない

AIは、

- 真偽
- 妥当性
- 現実整合性

を **評価関数として持たない**。

あるのは：

> **文脈上、もっとも尤もらしいかどうか**

そのため、

- 実在しそうな用語
- 過去に見たことがある構文
- 一貫した文章

が、**事実より優先されることがある**。

---

## ハルシネーションは「異常」ではない

重要な整理：

**ハルシネーションはAIが壊れた結果ではない。設計された確率空間の自然な帰結である。**

- 制約が弱ければ起きる
- 空間が汚れていれば起きる
- 判断を任せれば起きる

---

## なぜ「RAGを大量に入れると悪化することがあるのか」

直感と逆だが、数学的には自然。

- RAGが増える  
→ 確率空間が広がる  
→ ノイズ候補が増える  
→ 事実の相対確率が下がる  

**情報量の増加＝精度向上ではない**

---

## ハルシネーションを防ぐ唯一の方向性

本質的対策は1つだけ。

> **確率空間から  
> ハルシネーション候補を  
> 物理的に消すこと**

具体的には：

- 推測禁止
- 不明回答の明示許可
- 参照範囲の限定
- 判断の人間移譲

※ これは「対策」だが、  
ここでは詳細には踏み込まない。

---

## 一文でまとめる

> **ハルシネーションとは、  
> AIが嘘をついたのではなく、  
> 広すぎる確率空間の中で  
> “それらしいが間違った候補”を  
> 正しく選んでしまった結果である。**

---